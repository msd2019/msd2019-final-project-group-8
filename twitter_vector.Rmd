---
title: "Twitter_vector"
author: "Ankit Peshin"
date: "5/11/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

You can find the ShinyApp running here : hhttps://ankitpeshin.shinyapps.io/Twitter-wordvec/

The following chunk of code trains a word2vec model on the tweets provided with the Davidson et al. paper. The 50 dimensional word vector is then stored with an .Rdata extension (which is used to store objects from an Rdata workspace).

The words vector is then bundled with our Shiny app and deployed to the Server. The deployed app simply reads from this .Rdata file to restore the wordvector instead of training the model everytime it is started. 

```{r cars}

#Loading the relevant text2vec and dplyr libraries.

install.packages("text2vec")
library(text2vec)
library(dplyr)

#read labeled_data file from data folder
tweets = read_csv('data/labeled_data.csv')['tweet']
#filter out the tweets with @ , i.e handle name
tweets = tweets[ grep("@", tweets$tweet, invert = TRUE) , ]
tweets = tweets[ grep("&", tweets$tweet, invert = TRUE) , ]
#tweets are tokenized using the space tokenizer method
tokens <- space_tokenizer(as.character(tweets))
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)

vectorizer <- vocab_vectorizer(vocab)
# # use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)

word_vectors = glove$fit_transform(tcm, n_iter = 20)
word_vectors1 <- glove$components
save(word_vectors,file = "wvv.Rdata")

```



